{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c271ff6-e808-49ce-88db-c1952236b295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shaikh\n",
      "[nltk_data]     Irfan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Shaikh\n",
      "[nltk_data]     Irfan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install nltk if not already installed\n",
    "# !pip install nltk\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # For multilingual WordNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee8ef5-3fb5-48d3-8c15-376be3eac3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to analyze\n",
    "words = ['dog', 'cat', 'park']\n",
    "\n",
    "# Function to find semantic relationships\n",
    "def find_semantic_relations(word):\n",
    "    synsets = wn.synsets(word)\n",
    "    if not synsets:\n",
    "        print(f\"No synsets found for {word}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nWord: {word}\")\n",
    "    print(f\"Definition: {synsets[0].definition()}\")\n",
    "    \n",
    "    # Synonyms\n",
    "    synonyms = set()\n",
    "    for syn in synsets:\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    print(f\"Synonyms: {synonyms}\")\n",
    "    \n",
    "    # Antonyms\n",
    "    antonyms = set()\n",
    "    for syn in synsets:\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonyms.add(lemma.antonyms()[0].name())\n",
    "    print(f\"Antonyms: {antonyms}\")\n",
    "    \n",
    "    # Hypernyms (more general)\n",
    "    hypernyms = synsets[0].hypernyms()\n",
    "    print(f\"Hypernyms: {[h.name().split('.')[0] for h in hypernyms]}\")\n",
    "    \n",
    "    # Hyponyms (more specific)\n",
    "    hyponyms = synsets[0].hyponyms()\n",
    "    print(f\"Hyponyms: {[h.name().split('.')[0] for h in hyponyms]}\")\n",
    "    \n",
    "    # Meronyms (part-of relation)\n",
    "    meronyms = synsets[0].part_meronyms()\n",
    "    print(f\"Meronyms (Parts): {[m.name().split('.')[0] for m in meronyms]}\")\n",
    "    \n",
    "    # Holonyms (whole-of relation)\n",
    "    holonyms = synsets[0].member_holonyms()\n",
    "    print(f\"Holonyms (Wholes): {[h.name().split('.')[0] for h in holonyms]}\")\n",
    "\n",
    "# Apply to each word\n",
    "for word in words:\n",
    "    find_semantic_relations(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b897cd1c-e0b8-46a1-aff3-b21a3a5ed355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shaikh\n",
      "[nltk_data]     Irfan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Shaikh\n",
      "[nltk_data]     Irfan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c10f3dd9-40f7-475c-9fce-f7b2c98c369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['dog','cat','cap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c7be960a-1014-4f33-8096-0f8c5d0dd005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word: dog\n",
      "Definition: a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "Synonyms: {'wienerwurst', 'hotdog', 'firedog', 'hot_dog', 'heel', 'bounder', 'track', 'frankfurter', 'chase', 'cad', 'frank', 'go_after', 'blackguard', 'hound', 'trail', 'give_chase', 'Canis_familiaris', 'wiener', 'andiron', 'dog', 'tail', 'pawl', 'chase_after', 'frump', 'domestic_dog', 'click', 'weenie', 'detent', 'tag', 'dog-iron'}\n",
      "Antonyms: set()\n",
      "Hypernyms: ['canine', 'domestic_animal']\n",
      "Hyponyms: ['basenji', 'corgi', 'cur', 'dalmatian', 'great_pyrenees', 'griffon', 'hunting_dog', 'lapdog', 'leonberg', 'mexican_hairless', 'newfoundland', 'pooch', 'poodle', 'pug', 'puppy', 'spitz', 'toy_dog', 'working_dog']\n",
      "Meronyms: ['flag']\n",
      "Holonyms: ['canis', 'pack']\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    synsets = wn.synsets(word)\n",
    "    \n",
    "    print(f\"\\nword: {word}\")\n",
    "    print(f\"Definition: {synsets[0].definition()}\")\n",
    "    \n",
    "    synonyms=set()\n",
    "    antonyms=set()\n",
    "    for syn in synsets:\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonyms.add(lemma.antonyms()[0].name())\n",
    "            synonyms.add(lemma.name())\n",
    "    print(f\"Synonyms: {synonyms}\")\n",
    "    print(f\"Antonyms: {antonyms}\")\n",
    "\n",
    "    hypernyms = synsets[0].hypernyms()\n",
    "    print(f\"Hypernyms: {[h.name().split('.')[0] for h in hypernyms]}\")\n",
    "    hyponyms = synsets[0].hyponyms()\n",
    "    print(f\"Hyponyms: {[h.name().split('.')[0] for h in hyponyms]}\")\n",
    "    meronyms = synsets[0].part_meronyms()\n",
    "    print(f\"Meronyms: {[m.name().split('.')[0] for m in meronyms]}\")\n",
    "    holonyms = synsets[0].member_holonyms()\n",
    "    print(f\"Holonyms: {[h.name().split('.')[0] for h in holonyms]}\")\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a07fbdd-29f9-440d-8dd5-86603be2b7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Shaikh\n",
      "[nltk_data]     Irfan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Shaikh\n",
      "[nltk_data]     Irfan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2903ec2f-7b14-49e0-b3a5-735674b5e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "words = ['dog','cat','cap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd4f42ca-0828-4f68-aebb-c4e9b1bb6b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set() {'wiener', 'chase_after', 'domestic_dog', 'frankfurter', 'bounder', 'andiron', 'tail', 'hot_dog', 'pawl', 'cad', 'track', 'weenie', 'blackguard', 'go_after', 'detent', 'frump', 'firedog', 'trail', 'tag', 'click', 'hound', 'hotdog', 'frank', 'chase', 'Canis_familiaris', 'heel', 'give_chase', 'dog', 'dog-iron', 'wienerwurst'}\n",
      "Hypernyms: ['canine', 'domestic_animal']\n",
      "Hyponyms: ['basenji', 'corgi', 'cur', 'dalmatian', 'great_pyrenees', 'griffon', 'hunting_dog', 'lapdog', 'leonberg', 'mexican_hairless', 'newfoundland', 'pooch', 'poodle', 'pug', 'puppy', 'spitz', 'toy_dog', 'working_dog']\n",
      "{'vomit'} {'throw_up', 'regurgitate', 'cat', 'sick', 'computerized_tomography', 'CAT', 'retch', 'disgorge', 'Caterpillar', 'vomit', 'chuck', 'kat', 'spue', 'big_cat', 'honk', 'true_cat', 'be_sick', 'guy', 'quat', 'spew', 'purge', 'khat', 'qat', 'computerized_axial_tomography', 'bozo', 'barf', 'puke', 'computed_axial_tomography', 'regorge', \"cat-o'-nine-tails\", 'CT', 'hombre', 'vomit_up', 'cast', 'African_tea', 'Arabian_tea', 'upchuck', 'computed_tomography'}\n",
      "Hypernyms: ['feline']\n",
      "Hyponyms: ['domestic_cat', 'wildcat']\n",
      "set() {'ceiling', 'cap', 'crown', 'jacket_crown', 'detonating_device', 'chapiter', 'crest', 'crownwork', 'hood', 'roof', 'pileus', 'capital', 'detonator', 'jacket'}\n",
      "Hypernyms: ['headdress']\n",
      "Hyponyms: ['balaclava', 'balmoral', 'baseball_cap', 'bathing_cap', 'beret', 'biggin', 'biretta', 'calpac', 'cloth_cap', 'cockscomb', 'coonskin_cap', 'fez', 'garrison_cap', 'glengarry', 'kalansuwa', 'kepi', 'liberty_cap', 'mobcap', 'mortarboard', 'nightcap', 'pinner', 'sailor_cap', 'shower_cap', 'ski_cap', 'skullcap', 'tam', 'watch_cap', 'wishing_cap']\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    synsets = wn.synsets(word)\n",
    "\n",
    "\n",
    "    antonyms = set()\n",
    "    synonyms = set()\n",
    "    for syn in synsets:\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonyms.add(lemma.name())\n",
    "\n",
    "            synonyms.add(lemma.name())\n",
    "\n",
    "    print(antonyms,synonyms)\n",
    "\n",
    "    hypernyms = synsets[0].hypernyms()\n",
    "    print(f\"Hypernyms: {[h.name().split('.')[0] for h in hypernyms]}\")\n",
    "\n",
    "    hyponyms = synsets[0].hyponyms()\n",
    "    print(f\"Hyponyms: {[h.name().split('.')[0] for h in hyponyms]}\")\n",
    "\n",
    "    meronyms = synsets[0].part_meronyms()\n",
    "    print(f\"Meronyms: {[m.name().split('.')[0] for m in meronyms]}\")\n",
    "\n",
    "    holonyms = synsets[0].member_holonyms()\n",
    "    print(f\"Holonyms: \")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155bf8d6-94bf-4155-9235-8855f50fd644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
